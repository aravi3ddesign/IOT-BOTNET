{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pyspark\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, col\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "#from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-0DFIU6N:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Major Project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1eda84df550>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark= SparkSession.builder. \\\n",
    "appName(\"Major Project\").\\\n",
    "master(\"local[1]\").\\\n",
    "config(\"spark.memory.offHeap.enabled\",\"true\"). \\\n",
    "config(\"spark.memory.offHeap.size\",\"10g\").getOrCreate()\n",
    "spark\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.csv(r\"C:/Users/abhi/Desktop/project feb2023/major project code/small dataset/train2.csv\",inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print('Number of rows: \\t', df.count())\n",
    "print('Number of columns: \\t', len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#df.show(10)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bidirectional_duration_ms: integer (nullable = true)\n",
      " |-- bidirectional_packets: integer (nullable = true)\n",
      " |-- bidirectional_bytes: integer (nullable = true)\n",
      " |-- src2dst_first_seen_ms: long (nullable = true)\n",
      " |-- src2dst_last_seen_ms: long (nullable = true)\n",
      " |-- src2dst_packets: integer (nullable = true)\n",
      " |-- src2dst_bytes: integer (nullable = true)\n",
      " |-- dst2src_first_seen_ms: long (nullable = true)\n",
      " |-- dst2src_last_seen_ms: long (nullable = true)\n",
      " |-- dst2src_packets: integer (nullable = true)\n",
      " |-- dst2src_bytes: integer (nullable = true)\n",
      " |-- dst2src_duration_ms: integer (nullable = true)\n",
      " |-- src2dst_duration_ms: integer (nullable = true)\n",
      " |-- Target: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dataype of input data \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|Target|  count|\n",
      "+------+-------+\n",
      "|     1|6637795|\n",
      "|     0|    555|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Target').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "for col in df.columns:\n",
    "    df = df.withColumn(col, df[col].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StandardScaler, VectorAssembler, StringIndexer, Imputer\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe_stages= []\n",
    "\n",
    "# create the un-standardized features vector\n",
    "featureCols= ['bidirectional_duration_ms','bidirectional_packets','bidirectional_bytes',\n",
    "              'src2dst_packets','src2dst_bytes','dst2src_packets','dst2src_bytes',\n",
    "              'dst2src_duration_ms','src2dst_duration_ms']\n",
    "assembler= VectorAssembler(inputCols= featureCols,\n",
    "                           outputCol= \"features\",\n",
    "                           handleInvalid=\"keep\")\n",
    "pipe_stages += [assembler]\n",
    "\n",
    "# scale all features. \n",
    "ss= StandardScaler(inputCol=\"features\",\n",
    "                   outputCol=\"features_scale\",\n",
    "                   withMean= False,\n",
    "                   withStd=True)\n",
    "pipe_stages += [ss]\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"Target\", featuresCol=\"features_scale\", seed = 8464,\n",
    "                            numTrees=10, cacheNodeIds = True, subsamplingRate = 0.7)\n",
    "pipe_stages += [rf]\n",
    "\n",
    "pipe= Pipeline(stages= pipe_stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = pipe.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = mod.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.stages[-1].featureImportances\n",
    "#df= pipe.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractFeatureImp(featureImp, dataset, featuresCol):\n",
    "    list_extract = []\n",
    "    for i in dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "        list_extract = list_extract + dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "    varlist = pd.DataFrame(list_extract)\n",
    "    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "    return(varlist.sort_values('score', ascending = False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ExtractFeatureImp(mod.stages[-1].featureImportances, df2, \"features\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varlist = ExtractFeatureImp(mod.stages[-1].featureImportances, df2, \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varidx = [x for x in varlist['idx'][0:4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varidx  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_stages= []\n",
    "\n",
    "# create the un-standardized features vector\n",
    "featureCols= ['bidirectional_duration_ms','bidirectional_packets','bidirectional_bytes',\n",
    "              'src2dst_packets','src2dst_bytes','dst2src_packets','dst2src_bytes',\n",
    "              'dst2src_duration_ms','src2dst_duration_ms']\n",
    "assembler= VectorAssembler(inputCols= featureCols,\n",
    "                           outputCol= \"features\",\n",
    "                           handleInvalid=\"keep\")\n",
    "pipe_stages += [assembler]\n",
    "\n",
    "# scale all features. \n",
    "ss= StandardScaler(inputCol=\"features\",\n",
    "                   outputCol=\"features_scale\",\n",
    "                   withMean= False,\n",
    "                   withStd=True)\n",
    "pipe_stages += [ss]\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"Target\", featuresCol=\"features_scale\", seed = 8464,\n",
    "                            numTrees=10, cacheNodeIds = True, subsamplingRate = 0.7)\n",
    "pipe_stages += [rf]\n",
    "\n",
    "pipe= Pipeline(stages= pipe_stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " reference\n",
    "    \n",
    "    \n",
    "    \n",
    "#https://github.com/timlrx/timlrx.com/blob/master/data/blog/2018-06-19-\n",
    "#feature-selection-using-feature-importance-score-creating-a-pyspark-estimator.md\n",
    "\n",
    "\n",
    "#https://github.com/rampanyam/Predict-Next-Day-Rain-in-Australia-Classification\n",
    "#-Problem/blob/main/Predicting_Rain_Tomorrow_Kaggle_Raghuram.ipynb\n",
    "\n",
    "\n",
    "#https://haya-toumy.gitbook.io/spark-notes/pyspark/pyspark/random-forest-classifier-example\n",
    "#Creating a Pipeline to Do All Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
